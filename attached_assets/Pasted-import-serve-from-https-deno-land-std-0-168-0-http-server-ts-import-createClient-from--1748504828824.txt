import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';
const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
  'Access-Control-Allow-Methods': 'POST, GET, OPTIONS',
  'Access-Control-Max-Age': '86400'
};
const BATCH_SIZE = 100; // Process in batches of 100 leads
const MAX_EXECUTION_TIME = 50000; // 50 seconds (Edge Functions timeout at 60s)
serve(async (req)=>{
  // Handle CORS preflight requests
  if (req.method === 'OPTIONS') {
    return new Response(null, {
      status: 200,
      headers: corsHeaders
    });
  }
  const responseHeaders = {
    ...corsHeaders,
    'Content-Type': 'application/json'
  };
  try {
    console.log('üöÄ CSV Import Edge Function started');
    if (req.method !== 'POST') {
      return new Response(JSON.stringify({
        error: 'Method not allowed'
      }), {
        status: 405,
        headers: responseHeaders
      });
    }
    // Parse request body
    let body1;
    try {
      body1 = await req.json();
    } catch (error) {
      console.error('‚ùå Invalid JSON in request body:', error);
      return new Response(JSON.stringify({
        error: 'Invalid JSON in request body'
      }), {
        status: 400,
        headers: responseHeaders
      });
    }
    const { csvData, mappings, duplicateConfig, teamId, userId, jobId } = body1;
    // Validate required fields
    if (!csvData || !mappings || !teamId || !userId) {
      return new Response(JSON.stringify({
        error: 'Missing required fields',
        required: [
          'csvData',
          'mappings',
          'teamId',
          'userId'
        ]
      }), {
        status: 400,
        headers: responseHeaders
      });
    }
    console.log(`üìä Processing ${csvData.length} rows for team ${teamId}`);
    console.log(`üîÑ Duplicate config: ${duplicateConfig.duplicateDetectionField} -> ${duplicateConfig.duplicateAction}`);
    // Initialize Supabase Admin Client
    const supabaseAdmin = createClient(Deno.env.get('SUPABASE_URL') ?? '', Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? '');
    if (!Deno.env.get('SUPABASE_URL') || !Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')) {
      console.error('‚ùå Missing Supabase environment variables');
      return new Response(JSON.stringify({
        error: 'Server configuration error'
      }), {
        status: 500,
        headers: responseHeaders
      });
    }
    // Standard field names that map directly to database columns
    const standardFields = [
      'name',
      'email',
      'phone',
      'website',
      'address',
      'description',
      'status',
      'owner_id'
    ];
    let totalProcessed = 0;
    let totalNewRecords = 0;
    let totalUpdatedRecords = 0;
    let totalFailedRecords = 0;
    const allErrors = [];
    const startTime = Date.now();
    // Process data in batches
    const totalBatches = Math.ceil(csvData.length / BATCH_SIZE);
    console.log(`üì¶ Processing ${csvData.length} leads in ${totalBatches} batches of ${BATCH_SIZE}`);
    for(let batchIndex = 0; batchIndex < totalBatches; batchIndex++){
      // Check if we're approaching timeout
      const elapsedTime = Date.now() - startTime;
      if (elapsedTime > MAX_EXECUTION_TIME) {
        console.log(`‚è∞ Approaching timeout after ${elapsedTime}ms, stopping at batch ${batchIndex + 1}/${totalBatches}`);
        break;
      }
      const batchStart = batchIndex * BATCH_SIZE;
      const batchEnd = Math.min(batchStart + BATCH_SIZE, csvData.length);
      const currentBatch = csvData.slice(batchStart, batchEnd);
      console.log(`üì¶ Processing batch ${batchIndex + 1}/${totalBatches}: rows ${batchStart + 1}-${batchEnd}`);
      let batchProcessed = 0;
      let batchNewRecords = 0;
      let batchUpdatedRecords = 0;
      let batchFailedRecords = 0;
      const batchErrors = [];
      // Process each row in the current batch
      for(let i = 0; i < currentBatch.length; i++){
        try {
          const row = currentBatch[i];
          const rowNumber = batchStart + i + 1;
          // Build lead object from mappings
          const leadData = {
            team_id: teamId,
            custom_fields: {}
          };
          // Process each mapping
          mappings.forEach((mapping, mappingIndex)=>{
            if (mapping.fieldName && mappingIndex < row.length) {
              const value = row[mappingIndex]?.trim();
              if (!value) return; // Skip empty values
              if (standardFields.includes(mapping.fieldName)) {
                // Handle standard database fields
                if (mapping.fieldName === 'status') {
                  const validStatuses = [
                    'potential',
                    'contacted',
                    'qualified',
                    'closed'
                  ];
                  if (validStatuses.includes(value.toLowerCase())) {
                    leadData[mapping.fieldName] = value.toLowerCase();
                  } else {
                    leadData[mapping.fieldName] = 'potential';
                  }
                } else {
                  leadData[mapping.fieldName] = value;
                }
              } else {
                // Handle custom fields
                leadData.custom_fields[mapping.fieldName] = value;
              }
            }
          });
          // Ensure required fields and set defaults
          if (!leadData.name || !leadData.name.trim()) {
            batchErrors.push(`Row ${rowNumber}: Missing required 'name' field`);
            batchFailedRecords++;
            continue;
          }
          if (!leadData.status || ![
            'potential',
            'contacted',
            'qualified',
            'closed'
          ].includes(leadData.status)) {
            leadData.status = 'potential';
          }
          if (!leadData.team_id) {
            batchErrors.push(`Row ${rowNumber}: Missing team_id`);
            batchFailedRecords++;
            continue;
          }
          // Handle duplicate detection
          let existingLead = null;
          if (duplicateConfig.duplicateDetectionField !== 'none' && leadData[duplicateConfig.duplicateDetectionField]) {
            const { data: duplicateCheck, error: duplicateError } = await supabaseAdmin.from('leads').select('id, name, email, phone, custom_fields').eq('team_id', teamId).eq(duplicateConfig.duplicateDetectionField, leadData[duplicateConfig.duplicateDetectionField]).single();
            if (duplicateError && duplicateError.code !== 'PGRST116') {
              batchErrors.push(`Row ${rowNumber}: Duplicate check failed - ${duplicateError.message}`);
              batchFailedRecords++;
              continue;
            }
            if (duplicateCheck) {
              existingLead = duplicateCheck;
            }
          }
          // Handle duplicate action
          if (existingLead) {
            if (duplicateConfig.duplicateAction === 'skip') {
              batchProcessed++;
              continue;
            } else if (duplicateConfig.duplicateAction === 'update') {
              // Merge custom fields
              const mergedCustomFields = {
                ...existingLead.custom_fields || {},
                ...leadData.custom_fields
              };
              const updateData = {
                ...leadData,
                custom_fields: mergedCustomFields,
                updated_at: new Date().toISOString()
              };
              delete updateData.team_id; // Don't update team_id
              const { error: updateError } = await supabaseAdmin.from('leads').update(updateData).eq('id', existingLead.id);
              if (updateError) {
                batchErrors.push(`Row ${rowNumber}: Update failed - ${updateError.message}`);
                batchFailedRecords++;
              } else {
                batchUpdatedRecords++;
                batchProcessed++;
              }
              continue;
            }
          // For 'create_new', continue with normal insert
          }
          // Insert new lead
          if (!leadData.custom_fields || typeof leadData.custom_fields !== 'object') {
            leadData.custom_fields = {};
          }
          // Sanitize all field values
          const sanitizedLeadData = {
            team_id: leadData.team_id,
            name: String(leadData.name).trim(),
            email: leadData.email ? String(leadData.email).trim() : null,
            phone: leadData.phone ? String(leadData.phone).trim() : null,
            website: leadData.website ? String(leadData.website).trim() : null,
            address: leadData.address ? String(leadData.address).trim() : null,
            description: leadData.description ? String(leadData.description).trim() : null,
            status: leadData.status || 'potential',
            owner_id: leadData.owner_id || null,
            custom_fields: leadData.custom_fields,
            import_job_id: jobId || null
          };
          const { data: insertedLead, error: insertError } = await supabaseAdmin.from('leads').insert([
            sanitizedLeadData
          ]).select('id, name, status, team_id').single();
          if (insertError) {
            // Try fallback: Insert without custom_fields if that's the issue
            if (insertError.code === '23502' || insertError.message?.includes('custom_fields')) {
              const fallbackData = {
                ...sanitizedLeadData
              };
              delete fallbackData.custom_fields;
              const { data: fallbackLead, error: fallbackError } = await supabaseAdmin.from('leads').insert([
                fallbackData
              ]).select('id, name, status, team_id').single();
              if (fallbackError) {
                batchErrors.push(`Row ${rowNumber}: Insert failed - ${insertError.message} (Code: ${insertError.code})`);
                batchFailedRecords++;
              } else {
                batchNewRecords++;
                batchProcessed++;
              }
            } else {
              batchErrors.push(`Row ${rowNumber}: Insert failed - ${insertError.message} (Code: ${insertError.code})`);
              batchFailedRecords++;
            }
          } else {
            batchNewRecords++;
            batchProcessed++;
          }
        } catch (error) {
          const rowNumber = batchStart + i + 1;
          batchErrors.push(`Row ${rowNumber}: Unexpected error - ${error.message}`);
          batchFailedRecords++;
        }
      }
      // Update totals
      totalProcessed += batchProcessed;
      totalNewRecords += batchNewRecords;
      totalUpdatedRecords += batchUpdatedRecords;
      totalFailedRecords += batchFailedRecords;
      allErrors.push(...batchErrors);
      // Update progress in database after each batch
      if (jobId) {
        try {
          const progressPercent = Math.round(totalProcessed / csvData.length * 100);
          const progressUpdate = {
            processed_records: totalProcessed,
            failed_records: totalFailedRecords,
            status: totalProcessed >= csvData.length ? totalFailedRecords === 0 ? 'completed' : 'completed_with_errors' : 'processing',
            error_details: {
              summary: `Batch ${batchIndex + 1}/${totalBatches} completed: ${totalProcessed}/${csvData.length} processed`,
              new_records: totalNewRecords,
              updated_records: totalUpdatedRecords,
              failed_records: totalFailedRecords,
              progress_percent: progressPercent,
              current_batch: batchIndex + 1,
              total_batches: totalBatches,
              errors: allErrors.length > 0 ? allErrors.slice(-50) : undefined // Keep last 50 errors
            },
            updated_at: new Date().toISOString()
          };
          // Add completed_at if finished
          if (totalProcessed >= csvData.length) {
            progressUpdate.completed_at = new Date().toISOString();
          }
          await supabaseAdmin.from('import_jobs').update(progressUpdate).eq('id', jobId);
          console.log(`‚úÖ Progress updated: ${progressPercent}% (${totalProcessed}/${csvData.length})`);
        } catch (updateError) {
          console.error('‚ùå Failed to update progress:', updateError);
        }
      }
      console.log(`‚úÖ Batch ${batchIndex + 1}/${totalBatches} completed: +${batchNewRecords} new, +${batchUpdatedRecords} updated, +${batchFailedRecords} failed`);
    }
    // Final status update
    if (jobId) {
      try {
        const finalStatus = totalFailedRecords === 0 ? 'completed' : 'completed_with_errors';
        const finalErrorDetails = {
          summary: `Import completed: ${totalProcessed} processed, ${totalNewRecords} new, ${totalUpdatedRecords} updated, ${totalFailedRecords} failed`,
          new_records: totalNewRecords,
          updated_records: totalUpdatedRecords,
          failed_records: totalFailedRecords,
          progress_percent: 100,
          total_batches: totalBatches,
          errors: allErrors.length > 0 ? allErrors : undefined
        };
        const finalUpdate = {
          status: finalStatus,
          processed_records: totalProcessed,
          failed_records: totalFailedRecords,
          total_records: csvData.length,
          error_details: finalErrorDetails,
          completed_at: new Date().toISOString(),
          updated_at: new Date().toISOString()
        };
        await supabaseAdmin.from('import_jobs').update(finalUpdate).eq('id', jobId);
        console.log(`‚úÖ Import job ${jobId} final update completed with status: ${finalStatus}`);
      } catch (error) {
        console.error('‚ùå Failed to update final import job status:', error);
      }
    }
    // Prepare response
    const response = {
      success: true,
      processedRecords: totalProcessed,
      newRecords: totalNewRecords,
      updatedRecords: totalUpdatedRecords,
      failedRecords: totalFailedRecords,
      totalRows: csvData.length,
      jobId: jobId,
      batchesProcessed: Math.ceil(totalProcessed / BATCH_SIZE),
      executionTime: Date.now() - startTime,
      errors: allErrors.length > 0 ? allErrors.slice(-20) : undefined // Return last 20 errors
    };
    console.log('üìä Final Import Summary:', response);
    return new Response(JSON.stringify(response), {
      status: 200,
      headers: responseHeaders
    });
  } catch (error) {
    console.error('‚ùå Critical Edge Function error:', error);
    // Try to update import job with error status
    if (body?.jobId) {
      try {
        const supabaseAdmin = createClient(Deno.env.get('SUPABASE_URL') ?? '', Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? '');
        await supabaseAdmin.from('import_jobs').update({
          status: 'failed',
          error_details: {
            summary: 'Import failed due to critical error',
            error: error.message,
            stack: error.stack
          },
          completed_at: new Date().toISOString(),
          updated_at: new Date().toISOString()
        }).eq('id', body.jobId);
      } catch (jobError) {
        console.error('‚ùå Failed to update job with error status:', jobError);
      }
    }
    return new Response(JSON.stringify({
      success: false,
      error: 'Internal server error',
      message: error.message
    }), {
      status: 500,
      headers: responseHeaders
    });
  }
});
